{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3: Parallel Programming\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Time to break out the big guns. If your code is still taking too long and you've picked all the low-hanging fruit, it's time to think about parallel programming. \n",
    "\n",
    "Modern computers get their speed from having multiple cores, from the four or so in your laptop, to thousands in a supercomputer. But your code doesn't usually 'know' how to take advantage of these resources by itself -- you have to figure out how to break up your code into pieces that can run on different cores at the same time.\n",
    "\n",
    "This can range from easy to exceedingly complicated depending on task requirements and scale. But often you can get by with some simple tricks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parallel Programming Approaches\n",
    "\n",
    "* Embarassingly Parallel\n",
    "* Multicore\n",
    "* Multinode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Embarassingly Parallel\n",
    "\n",
    "<center><img src=\"embarassing.png\" width=\"40%\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Embarassingly parallel is where each unit of computation is completely independent of the others. For instance, if you're batch processing a set of images and you only care about one image at a time.\n",
    "\n",
    "This is the easiest case to deal with! Perhaps you have a bunch of files to process? Or you need to run a model across a range of a range of parameters, and each run is independent? We can run one job per computer, or perhaps a couple per computer (but isolated from one other) to get the speed we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Embarassingly Parallel in Python\n",
    "\n",
    "_Simplest_: Just run multiple Python instances at the same time (e.g. one per dataset).\n",
    "\n",
    "```\n",
    "python my_code.py data_1.dat &\n",
    "python my_code.py data_2.dat &\n",
    "python my_code.py data_3.dat &\n",
    "python my_code.py data_4.dat &\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This is the easiest approach -- just run multiple copies of your program at once with different inputs. The operating system will distribute these over the multiple cores of your computer, but keep in mind that if you create more instances than cores in your computer (typically 2-4) you mightn't see an additional speedup. Memory also has to be shared between the different processes. \n",
    "\n",
    "Embarassingly parallel includes not just running multiple jobs on the same computer, but also across multiple computers. We'll talk about how to do this on a supercomputer later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Embarassingly Parallel in Python\n",
    "\n",
    "Little More Advanced: Use the Python `multiprocessing` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def process_file(filename):\n",
    "    data = open(filename).read()\n",
    "    output = my_analysis(data)\n",
    "    open('output.dat', 'w').write(output)\n",
    "\n",
    "pool = Pool(processes=8)\n",
    "input_files = ['data_1.dat', '...', 'data_8.dat']\n",
    "count = pool.map(process_file, input_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This code will spin up 8 processes, and run my_analysis on each and save the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multicore\n",
    "\n",
    "<center><img src=\"multicore.png\" width=\"20%\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Multicore is the next step up. This is where your program shares the load across the different cores (or CPUs) within your computer. Typically a program will only run on one core at a time, but by splitting it up into different processes or threads we can share the load. Your desktop computer probably has at least four cores, and a heavy-duty server might have 12 or more (even up to 64 or more in some specialised circumstances).\n",
    "\n",
    "Communication is all within the same computer and so communication is fast, and data can be readily shared between processes through memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Multicore in Python\n",
    "\n",
    "Let's use `multiprocessing` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Example extending above, where we summarise results in a final step (and so no longer embarassingly parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multinode\n",
    "\n",
    "<center><img src=\"multinode.png\" width=\"35%\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Multinode is the most advanced approach, but also the only way to scale up once you've used up all the resources in a single PC for a given job. Like multinode, we make use of all the cores in a given computer, but we also share the load across other computers, typically over a high-speed network. This is the essence of how a traditional supercomputer works, which we'll talk about later.\n",
    "\n",
    "Since we now have to communicate beyond a single PC, we can't use shared memory to communicate, instead we pass messages over a network, typically using a protocol called MPI -- (M)essage (P)assing (I)nterface. The only thing to be careful of is that sending messages over the network is usually slower than sending between cores, so we have to structure our problems carefully.\n",
    "\n",
    "There are some really important applications of multinode high performance computing, particularly complex physics simulations, and so it's handy to keep this in the back of your mind. But I'd say 90% of you only have to worry embarassingly parallel and multicore techniques to get your work done satisfactorily. \n",
    "\n",
    "Let's now talk about how you apply this in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Multinode in Python\n",
    "\n",
    "* MPI4Py - (M)essage (P)assing (I)nterface for Python (Low Level Interface)\n",
    "* Dask - NumPy arrays and Pandas Dataframes over Mulitple Computers (High Level Interface)\n",
    "* iPython Parallel - For interactive work like this presentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### MPI4Py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MPI4Py Demo here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dask demo here, inc. kicking off cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask can do much more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Which technique do I choose?\n",
    "\n",
    "<blockquote>I have a thousand images to process. Each one takes about 5 minutes, and they don't depend on one another.</blockquote>\n",
    "\n",
    "Embarassingly Parallel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<blockquote>I've written a simulation of coral growth. It doesn't take long to run, but I need to try it with many different combinations of parameters, which will take ages.</blockquote>\n",
    "\n",
    "Embarassingly Parallel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<blockquote>I have a genomics pipeline that includes a step where the data is split into chunks for processing before being recombined.</blockquote>\n",
    "\n",
    "Multicore (maybe multinode if big enough)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<blockquote>I'm simulating a jet turbine by modelling the physics and chemistry within discrete volumes of space which interact with adjoining volumes over time. The more volumes I can simulate the better!</blockquote>\n",
    "\n",
    "Multinode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges - some simple pseudo code, or some scenarios, and explain how you might make each parallel? "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
